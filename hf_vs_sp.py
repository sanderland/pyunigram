
import os
from datasets import load_dataset
from tokenizers import Tokenizer, models, pre_tokenizers, trainers
import sentencepiece as spm
from tqdm import tqdm

# Load the wikitext dataset
print("Loading wikitext training corpus...")
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")

# Helper to yield batches for training
def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]

# --- Train Hugging Face Unigram Tokenizer ---
print("\nTraining Hugging Face Unigram Tokenizer...")
hf_tokenizer = Tokenizer(models.Unigram())
hf_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
trainer = trainers.UnigramTrainer(
    vocab_size=20000, special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)
hf_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
hf_tokenizer_path = "hf_unigram_tokenizer.json"
hf_tokenizer.save(hf_tokenizer_path)
print(f"Hugging Face tokenizer trained and saved to {hf_tokenizer_path}")

# --- Train SentencePiece Unigram Tokenizer ---
print("\nTraining SentencePiece Unigram Tokenizer...")
corpus_file = "wikitext_corpus.txt"
with open(corpus_file, "w", encoding="utf-8") as f:
    for text_batch in get_training_corpus():
        for text in text_batch:
            f.write(text + "\n")
spm.SentencePieceTrainer.train(
    f'--input={corpus_file} --model_prefix=sp_unigram --vocab_size=20000 '
    '--model_type=unigram '
    '--character_coverage=1.0 '
    '--byte_fallback=true '
    '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '
    '--pad_piece=[PAD] --unk_piece=[UNK] --bos_piece=[CLS] --eos_piece=[SEP]'
)
sp_model_path = "sp_unigram.model"
print(f"SentencePiece tokenizer trained and model saved to {sp_model_path}")

# --- Load trained tokenizers for compression test ---
print("\nLoading trained tokenizers...")
hf_tokenizer = Tokenizer.from_file(hf_tokenizer_path)
sp = spm.SentencePieceProcessor()
sp.load(sp_model_path)
print("Tokenizers loaded successfully.")
dataset = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")

# Initialize counters for totals
total_bytes = 0
total_hf_tokens = 0
total_sp_tokens = 0

print("\nCalculating compression over the entire training corpus...")
for text in tqdm(dataset["text"], desc="Processing dataset"):
    if text.strip():
        total_bytes += len(text.encode('utf-8'))
        total_hf_tokens += len(hf_tokenizer.encode(text).ids)
        total_sp_tokens += len(sp.encode_as_ids(text))

# Calculate final compression ratios and report the results
print("\n--- Overall Compression Results ---")
print(f"Total Bytes in Corpus: {total_bytes:,}")
print("-" * 35)

if total_hf_tokens > 0:
    hf_compression = total_bytes / total_hf_tokens
    print("Hugging Face Unigram Tokenizer:")
    print(f"  Total Tokens: {total_hf_tokens:,}")
    print(f"  Compression Ratio (bytes/token): {hf_compression:.4f}")
else:
    print("No tokens were generated by the Hugging Face tokenizer.")

print("-" * 35)

if total_sp_tokens > 0:
    sp_compression = total_bytes / total_sp_tokens
    print("SentencePiece Unigram Tokenizer:")
    print(f"  Total Tokens: {total_sp_tokens:,}")
    print(f"  Compression Ratio (bytes/token): {sp_compression:.4f}")
else:
    print("No tokens were generated by the SentencePiece tokenizer.")

print("-" * 35)